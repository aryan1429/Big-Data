{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzhOPy5J2k9Zm2Ys5HExq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan1429/Big-Data/blob/main/MatrixMultiplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KpNXLYvF0iHj",
        "outputId": "48c95f0e-de20-4576-ff91-14b06cbae7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless > /dev/null\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "!mv hadoop-3.3.6 /usr/local/hadoop\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin:{os.environ['HADOOP_HOME']}/sbin\"\n",
        "\n",
        "!hadoop version\n"
      ],
      "metadata": {
        "id": "qrcooCm011II",
        "outputId": "3fc1824a-b72e-4c19-9021-65af95e700e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"This is a sample file for Hadoop testing.\" > sample.txt\n",
        "!cat sample.txt"
      ],
      "metadata": {
        "id": "a3UREw3u3Apj",
        "outputId": "65f0f474-5edf-48be-d00c-f561e29d655c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample file for Hadoop testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_input.txt\n",
        "A,0,0,1\n",
        "A,0,1,2\n",
        "A,0,2,3\n",
        "A,1,0,4\n",
        "A,1,1,5\n",
        "A,1,2,6\n",
        "B,0,0,7\n",
        "B,0,1,8\n",
        "B,1,0,9\n",
        "B,1,1,10\n",
        "B,2,0,11\n",
        "B,2,1,12"
      ],
      "metadata": {
        "id": "i7S4DUtciTr_",
        "outputId": "7616505c-0e5d-4063-dced-792ca6610473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "n = 3\n",
        "p = 2\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "  if not line:\n",
        "    continue\n",
        "  name,i,j,val = line.split(',')\n",
        "  i,j,val = int(i),int(j),float(val)\n",
        "  if name == 'A':\n",
        "    for col in range(p):\n",
        "      print(f\"{i},{col}\\tA,{j},{val}\")\n",
        "  else:\n",
        "    for row in range(2):\n",
        "      print(f\"{row},{j}\\tB,{i},{val}\")"
      ],
      "metadata": {
        "id": "_8WVyCz4mQRV",
        "outputId": "38ae5923-6ae5-4c14-fa0e-670ab320c4c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "current_key = None\n",
        "A = defaultdict(float)\n",
        "B = defaultdict(float)\n",
        "def emit (key,A,B):\n",
        "  total = 0\n",
        "  for k in A:\n",
        "    total += A[k] * B.get(k,0)\n",
        "  print(f\"{key}\\t{total}\")\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "  if not line:\n",
        "    continue\n",
        "  key,val = line.split('\\t')\n",
        "  if current_key and key != current_key:\n",
        "    emit(current_key,A,B)\n",
        "    A = defaultdict(float)\n",
        "    B = defaultdict(float)\n",
        "  current_key = key\n",
        "  tag,k,v= val.split(',')\n",
        "  k,v = int(k),float(v)\n",
        "  if tag =='A':\n",
        "    A[k] = v\n",
        "  else:\n",
        "    B[k] = v\n",
        "if current_key:\n",
        "  emit(current_key,A,B)"
      ],
      "metadata": {
        "id": "NozVbyJBtqFT",
        "outputId": "0475cf39-c9c3-40a0-9e22-3d2b9ce12d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x mapper.py reducer.py"
      ],
      "metadata": {
        "id": "BBzIUgK-38lo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mkdir /matrix"
      ],
      "metadata": {
        "id": "_5D_vElWGp0N",
        "outputId": "6ad4d036-77bc-4d7b-c623-bff80e46d41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/matrix': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch matrix.txt\n"
      ],
      "metadata": {
        "id": "fCpIUZVQG7Kt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -put matrix.txt /matrix"
      ],
      "metadata": {
        "id": "XglLI9HNILoI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    -input matrix_input.txt \\\n",
        "    -output output_matrix \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py\n"
      ],
      "metadata": {
        "id": "IGSItjISInJF",
        "outputId": "7133db35-5dc3-4b36-b4eb-64b93cababd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-02 06:06:15,824 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-11-02 06:06:15,919 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-11-02 06:06:15,919 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-11-02 06:06:15,939 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-02 06:06:15,970 ERROR streaming.StreamJob: Error Launching job : Output directory file:/content/output_matrix already exists\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/output_matrix\n"
      ],
      "metadata": {
        "id": "jj9jVk4dJBOm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "-input /content/matrix_input.txt \\\n",
        "-output /content/output_matrix \\\n",
        "-mapper /content/mapper.py \\\n",
        "-reducer /content/reducer.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "--CwsCjGLTST",
        "outputId": "ab2aad59-4be1-4457-800e-f28bd0202dca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-02 06:07:13,986 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-11-02 06:07:14,095 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-11-02 06:07:14,095 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-11-02 06:07:14,131 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-02 06:07:14,441 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2025-11-02 06:07:14,470 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2025-11-02 06:07:14,706 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1043502234_0001\n",
            "2025-11-02 06:07:14,706 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-11-02 06:07:14,965 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-11-02 06:07:14,969 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-11-02 06:07:14,969 INFO mapreduce.Job: Running job: job_local1043502234_0001\n",
            "2025-11-02 06:07:14,974 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2025-11-02 06:07:14,989 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-02 06:07:14,989 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-02 06:07:15,067 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-11-02 06:07:15,072 INFO mapred.LocalJobRunner: Starting task: attempt_local1043502234_0001_m_000000_0\n",
            "2025-11-02 06:07:15,131 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-02 06:07:15,131 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-02 06:07:15,171 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-02 06:07:15,194 INFO mapred.MapTask: Processing split: file:/content/matrix_input.txt:0+99\n",
            "2025-11-02 06:07:15,217 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-11-02 06:07:15,357 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-11-02 06:07:15,357 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-11-02 06:07:15,357 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-11-02 06:07:15,357 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-11-02 06:07:15,357 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-11-02 06:07:15,361 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-11-02 06:07:15,365 INFO streaming.PipeMapRed: PipeMapRed exec [/content/mapper.py]\n",
            "2025-11-02 06:07:15,370 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2025-11-02 06:07:15,373 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2025-11-02 06:07:15,373 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2025-11-02 06:07:15,374 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2025-11-02 06:07:15,375 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2025-11-02 06:07:15,375 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2025-11-02 06:07:15,377 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2025-11-02 06:07:15,377 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2025-11-02 06:07:15,377 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2025-11-02 06:07:15,378 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2025-11-02 06:07:15,378 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-11-02 06:07:15,379 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2025-11-02 06:07:15,411 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-02 06:07:15,417 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-02 06:07:15,510 INFO streaming.PipeMapRed: Records R/W=12/1\n",
            "2025-11-02 06:07:15,524 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-02 06:07:15,526 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-02 06:07:15,533 INFO mapred.LocalJobRunner: \n",
            "2025-11-02 06:07:15,535 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-11-02 06:07:15,536 INFO mapred.MapTask: Spilling map output\n",
            "2025-11-02 06:07:15,536 INFO mapred.MapTask: bufstart = 0; bufend = 294; bufvoid = 104857600\n",
            "2025-11-02 06:07:15,536 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600\n",
            "2025-11-02 06:07:15,559 INFO mapred.MapTask: Finished spill 0\n",
            "2025-11-02 06:07:15,606 INFO mapred.Task: Task:attempt_local1043502234_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-11-02 06:07:15,614 INFO mapred.LocalJobRunner: Records R/W=12/1\n",
            "2025-11-02 06:07:15,615 INFO mapred.Task: Task 'attempt_local1043502234_0001_m_000000_0' done.\n",
            "2025-11-02 06:07:15,640 INFO mapred.Task: Final Counters for attempt_local1043502234_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141502\n",
            "\t\tFILE: Number of bytes written=783581\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=24\n",
            "\t\tMap output bytes=294\n",
            "\t\tMap output materialized bytes=348\n",
            "\t\tInput split bytes=82\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=24\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=99\n",
            "2025-11-02 06:07:15,640 INFO mapred.LocalJobRunner: Finishing task: attempt_local1043502234_0001_m_000000_0\n",
            "2025-11-02 06:07:15,642 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2025-11-02 06:07:15,650 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-11-02 06:07:15,650 INFO mapred.LocalJobRunner: Starting task: attempt_local1043502234_0001_r_000000_0\n",
            "2025-11-02 06:07:15,688 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-02 06:07:15,689 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-02 06:07:15,690 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-02 06:07:15,742 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7f4c8280\n",
            "2025-11-02 06:07:15,756 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-02 06:07:15,793 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2116865152, maxSingleShuffleLimit=529216288, mergeThreshold=1397131008, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-11-02 06:07:15,803 INFO reduce.EventFetcher: attempt_local1043502234_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-11-02 06:07:15,901 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1043502234_0001_m_000000_0 decomp: 344 len: 348 to MEMORY\n",
            "2025-11-02 06:07:15,910 INFO reduce.InMemoryMapOutput: Read 344 bytes from map-output for attempt_local1043502234_0001_m_000000_0\n",
            "2025-11-02 06:07:15,915 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 344, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->344\n",
            "2025-11-02 06:07:15,919 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-11-02 06:07:15,919 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-02 06:07:15,921 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-11-02 06:07:15,934 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-02 06:07:15,935 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 338 bytes\n",
            "2025-11-02 06:07:15,937 INFO reduce.MergeManagerImpl: Merged 1 segments, 344 bytes to disk to satisfy reduce memory limit\n",
            "2025-11-02 06:07:15,938 INFO reduce.MergeManagerImpl: Merging 1 files, 348 bytes from disk\n",
            "2025-11-02 06:07:15,940 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-11-02 06:07:15,940 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-02 06:07:15,942 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 338 bytes\n",
            "2025-11-02 06:07:15,943 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-02 06:07:15,947 INFO streaming.PipeMapRed: PipeMapRed exec [/content/reducer.py]\n",
            "2025-11-02 06:07:15,962 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2025-11-02 06:07:15,973 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2025-11-02 06:07:15,996 INFO mapreduce.Job: Job job_local1043502234_0001 running in uber mode : false\n",
            "2025-11-02 06:07:15,997 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2025-11-02 06:07:16,023 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-02 06:07:16,023 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-02 06:07:16,106 INFO streaming.PipeMapRed: Records R/W=24/1\n",
            "2025-11-02 06:07:16,120 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-02 06:07:16,121 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-02 06:07:16,123 INFO mapred.Task: Task:attempt_local1043502234_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-11-02 06:07:16,124 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-02 06:07:16,124 INFO mapred.Task: Task attempt_local1043502234_0001_r_000000_0 is allowed to commit now\n",
            "2025-11-02 06:07:16,126 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1043502234_0001_r_000000_0' to file:/content/output_matrix\n",
            "2025-11-02 06:07:16,127 INFO mapred.LocalJobRunner: Records R/W=24/1 > reduce\n",
            "2025-11-02 06:07:16,127 INFO mapred.Task: Task 'attempt_local1043502234_0001_r_000000_0' done.\n",
            "2025-11-02 06:07:16,128 INFO mapred.Task: Final Counters for attempt_local1043502234_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142230\n",
            "\t\tFILE: Number of bytes written=783979\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=4\n",
            "\t\tReduce shuffle bytes=348\n",
            "\t\tReduce input records=24\n",
            "\t\tReduce output records=4\n",
            "\t\tSpilled Records=24\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=33\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=50\n",
            "2025-11-02 06:07:16,128 INFO mapred.LocalJobRunner: Finishing task: attempt_local1043502234_0001_r_000000_0\n",
            "2025-11-02 06:07:16,128 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-11-02 06:07:17,005 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2025-11-02 06:07:17,006 INFO mapreduce.Job: Job job_local1043502234_0001 completed successfully\n",
            "2025-11-02 06:07:17,022 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283732\n",
            "\t\tFILE: Number of bytes written=1567560\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=24\n",
            "\t\tMap output bytes=294\n",
            "\t\tMap output materialized bytes=348\n",
            "\t\tInput split bytes=82\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=4\n",
            "\t\tReduce shuffle bytes=348\n",
            "\t\tReduce input records=24\n",
            "\t\tReduce output records=4\n",
            "\t\tSpilled Records=48\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=33\n",
            "\t\tTotal committed heap usage (bytes)=463470592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=99\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=50\n",
            "2025-11-02 06:07:17,023 INFO streaming.StreamJob: Output directory: /content/output_matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/output_matrix/part-00000\n"
      ],
      "metadata": {
        "id": "al53N1k_MENm",
        "outputId": "65a4c9a8-d0a6-4ec5-ad7f-2826e2bd3f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0,0\t58.0\n",
            "0,1\t64.0\n",
            "1,0\t139.0\n",
            "1,1\t154.0\n"
          ]
        }
      ]
    }
  ]
}